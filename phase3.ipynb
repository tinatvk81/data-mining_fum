{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\BlueWindows\\AppData\\Local\\Temp\\ipykernel_9028\\2842758783.py:4: DtypeWarning: Columns (12,29,30,33,35,37,38,40,42,43,44,46,47,48,49,50,52,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df4=pd.read_csv('Playstore_final.csv')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df4=pd.read_csv('Playstore_final.csv')\n",
        "\n",
        "df4 = df4.dropna(subset=['Rating Count', 'Minimum Android', 'Size', 'Installs'])\n",
        "\n",
        "df4['Size'] = df4['Size'].astype(str).str.replace('M', '').str.replace('Varies with device', 'NaN')\n",
        "df4['Size'] = df4['Size'].astype(str).str.replace('k', '').str.replace('+', '').str.replace(',', '')#.astype(float)\n",
        "df4['Size'] = df4['Size'].str.replace('NaN','')\n",
        "\n",
        "df4['Installs'] = df4['Installs'].astype(str).str.replace('+', '').str.replace(',', '')#.astype(str)\n",
        "\n",
        "df4['Minimum Android'] = df4['Minimum Android'].astype(str).str.replace('Varies with device', 'NaN')#.astype(str)\n",
        "df4['Minimum Android'] = df4['Minimum Android'].str.replace('NaN','')\n",
        "\n",
        "\n",
        "df4['Rating Count'] = df4['Rating Count'] .astype(str).str.replace('N/A', 'NaN')#.astype(str)\n",
        "df4['Rating Count'] =df4['Rating Count'] .str.replace('NaN','')\n",
        "\n",
        "df4['Size'] = pd.to_numeric(df4['Size'], errors='coerce')\n",
        "df4['Installs'] = pd.to_numeric(df4['Installs'], errors='coerce')\n",
        "df4['Rating Count'] = pd.to_numeric(df4['Rating Count'], errors='coerce')\n",
        "df4['Minimum Android'] = pd.to_numeric(df4['Minimum Android'], errors='coerce')\n",
        "\n",
        "\n",
        "df4 = df4.dropna(subset=['Rating Count', 'Minimum Android', 'Size', 'Installs'])\n",
        "\n",
        "numerical_columns=['Rating Count', 'Minimum Android','Minimum Installs', 'Size', 'Installs','Rating',\"Price\",'Reviews']\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "df4[numerical_columns]= imputer.fit_transform(df4[numerical_columns].astype(float))\n",
        "\n",
        "df4.to_csv('Playstore_final.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "Can only use .str accessor with string values!",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mType\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mType\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mFree\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNaN\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mType\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mType\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mNaN\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mPrice\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mEveryone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m$\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_numeric(df[\u001b[39m'\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m'\u001b[39m], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Convert to numeric, coercing errors to NaN\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdropna(subset\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mRating\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mReviews\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSize\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mInstalls\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mType\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPrice\u001b[39m\u001b[39m'\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\BlueWindows\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5987\u001b[0m ):\n\u001b[0;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
            "File \u001b[1;32mc:\\Users\\BlueWindows\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
            "File \u001b[1;32mc:\\Users\\BlueWindows\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:180\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[1;32m--> 180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[0;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n",
            "File \u001b[1;32mc:\\Users\\BlueWindows\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:234\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    231\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39minfer_dtype(values, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m inferred_dtype\n",
            "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv('GooglePlay.csv')\n",
        "\n",
        "df = df.dropna(subset=['Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price'])\n",
        "\n",
        "df['Size'] = df['Size'].astype(str).str.replace('M', '').str.replace('Varies with device', 'NaN')\n",
        "df['Size'] = df['Size'].astype(str).str.replace('k', '').str.replace('+', '').str.replace(',', '')#.astype(float)\n",
        "df['Size'] = df['Size'].str.replace('NaN','')\n",
        "\n",
        "df['Installs'] = df['Installs'].astype(str).str.replace('+', '').str.replace(',', '')#.astype(str)\n",
        "\n",
        "df['Type'] = df['Type'].astype(str).str.replace('Free', 'NaN')\n",
        "df['Type'] = df['Type'].str.replace('NaN','')\n",
        "\n",
        "df['Price'] = df['Price'].str.replace('Everyone', '').str.replace('$', '')\n",
        "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
        "\n",
        "\n",
        "df = df.dropna(subset=['Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price'])\n",
        "\n",
        "df.to_csv('GooglePlay.csv', index=False)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "section 4.combine 2 datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\BlueWindows\\AppData\\Local\\Temp\\ipykernel_9028\\1467324043.py:14: DtypeWarning: Columns (12,29,30,33,35,37,38,40,42,43,44,46,47,48,49,50,52,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df2 = pd.read_csv(excel2_path)\n"
          ]
        }
      ],
      "source": [
        "# # Define column names for each dataset (remove if already named)\n",
        "# dataset1_cols = [\"App\", \"Category\", \"Rating\", \"Reviews\", \"Size\", \"Installs\", \"Type\", \"Price\", \"Content Rating\", \"Genres\", \"Last Updated\", \"Current Ver\", \"Android Ver\"]\n",
        "# dataset2_cols = ['App Name', 'App Id', 'Category', 'Rating', 'Rating Count', 'Installs', 'Minimum Installs', 'Free', 'Price', 'Currency', 'Size', 'Minimum Android', 'Developer Id', 'Developer Website', 'Developer Email', 'Released', 'Last update', 'Privacy Policy', 'Content Rating', 'Ad Supported', 'In app purchases', 'Editor Choice', 'Summary', 'Reviews', 'Android version Text', 'Developer', 'Developer Address', 'Developer Internal ID', 'Version']\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load the first Excel file into a DataFrame\n",
        "excel1_path = 'GooglePlay.csv'\n",
        "df1 = pd.read_csv(excel1_path)\n",
        "\n",
        "# Load the second Excel file into a DataFrame\n",
        "excel2_path = 'Playstore_final.csv'\n",
        "df2 = pd.read_csv(excel2_path)\n",
        "\n",
        "# merged_df = pd.concat([df1, df2], ignore_index=False)\n",
        "df3 = pd.merge(df1, df2, how='inner', left_on='App', right_on='App Name')\n",
        "df3.drop(columns=['App Name'], inplace=True)\n",
        "# Save the merged DataFrame to CSV\n",
        "df3.to_csv('finals.csv', index=False)\n",
        "\n",
        "# Write the merged DataFrame to a new Excel file\n",
        "# merged_excel_path = 'finals.csv'\n",
        "# df3= pd.read_csv('finals.csv')\n",
        "# final_df.to_csv(merged_excel_path, index=False)\n",
        "\n",
        "columns_to_drop = [col for col in df3.columns if col.startswith('Unnamed:')]\n",
        "\n",
        "# Drop the columns\n",
        "df3.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Write the modified DataFrame to a new CSV file\n",
        "# output_csv_file = \"modified_final.csv\"\n",
        "df3.to_csv('finals.csv', index=False)\n",
        "\n",
        "\n",
        "# print(\"Merged data saved to:\", merged_excel_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # import pandas as pd\n",
        "\n",
        "# # Load the merged DataFrame from CSV\n",
        "# # merged_csv_path = \n",
        "# df = pd.read_csv('finals.csv')\n",
        "\n",
        "# # Check data types of columns\n",
        "# numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n",
        "# nominal_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# # Fill empty values in numeric columns with mean\n",
        "# for col in numeric_cols:\n",
        "#     if df[col].isnull().any():\n",
        "#         df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "# # Fill empty values in nominal columns with most frequent value\n",
        "# for col in nominal_cols:\n",
        "#     if df[col].isnull().any():\n",
        "#         mode_value = df[col].mode()[0]\n",
        "#         df[col].fillna(mode_value, inplace=True)\n",
        "\n",
        "# # Save the modified DataFrame to a new CSV file\n",
        "# modified_csv_path = 'finals_modified.csv'\n",
        "# df.to_csv(modified_csv_path, index=False)\n",
        "import pandas as pd\n",
        "\n",
        "# Load the merged DataFrame from CSV\n",
        "merged_csv_path = 'finals.csv'\n",
        "df = pd.read_csv(merged_csv_path)\n",
        "\n",
        "# Define a function to fill missing values based on column type\n",
        "def fill_missing_values(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            # For nominal columns, fill missing values with 'N/A'\n",
        "            df[col].fillna('N/A', inplace=True)\n",
        "        else:\n",
        "            # For numeric columns, fill missing values with the mean\n",
        "            df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "# Fill missing values\n",
        "fill_missing_values(df)\n",
        "\n",
        "# Write the modified DataFrame to a new CSV file\n",
        "output_csv_file = 'finals.csv'\n",
        "df.to_csv(output_csv_file, index=False)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "section 4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\BlueWindows\\AppData\\Local\\Temp\\ipykernel_9028\\2765317825.py:5: DtypeWarning: Columns (12,29,30,33,35,37,38,40,42,43,44,46,47,48,49,50,52,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df2 = pd.read_csv('Playstore_final.csv')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Read the first dataset\n",
        "df1 = pd.read_csv('finals.csv')\n",
        "\n",
        "# Read the second dataset\n",
        "df2 = pd.read_csv('Playstore_final.csv')\n",
        "\n",
        "# Columns to merge and handle\n",
        "# columns_to_merge = {\n",
        "#     'Category_x': 'Category',\n",
        "#     'Category_y': 'Category',\n",
        "#     'Rating_x': 'Rating',\n",
        "#     'Rating_y': 'Rating',\n",
        "#     'Reviews_x': 'Reviews',\n",
        "#     'Reviews_y': 'Reviews',\n",
        "#     'Size_x': 'Size',\n",
        "#     'Size_y': 'Size',\n",
        "#     'Installs_x': 'Installs',\n",
        "#     'Installs_y': 'Installs',\n",
        "#     'Price_x': 'Price',\n",
        "#     'Price_y': 'Price',\n",
        "#     'Content Rating_x': 'Content Rating',\n",
        "#     'Content Rating_y': 'Content Rating'\n",
        "# }\n",
        "\n",
        "columns_to_merge=['Category','Rating','Reviews','Size','Installs','Price','Content Rating']\n",
        "\n",
        "for i in columns_to_merge:\n",
        "    numeric_values_x = pd.to_numeric(df1[i+'_x'], errors='coerce')\n",
        "    numeric_values_y = pd.to_numeric(df1[i+'_y'], errors='coerce')\n",
        "    if numeric_values_x.notna().all() and numeric_values_y.notna().all():\n",
        "            # Calculate the mean if both columns contain numeric values\n",
        "            df1[i] = (numeric_values_x + numeric_values_y) / 2\n",
        "    else:\n",
        "            # If any of the columns contain non-numeric values, use values from the second dataset\n",
        "            df1[i] = df2[i].fillna('N/A')\n",
        "            # If both columns have non-numeric values, just copy from second dataset\n",
        "            # if numeric_values_x.isna().all() and numeric_values_y.isna().all():\n",
        "            #     df1[i] = df2[i].fillna('')\n",
        "    x=[i+'_y',i+'_x']\n",
        "    df1.drop(columns=x, inplace=True)\n",
        "    \n",
        "\n",
        "\n",
        "# for col_x, col_y in columns_to_merge.items():\n",
        "#     if col_x in df1.columns and col_y in df2.columns:\n",
        "#         # Check if both columns contain numeric values\n",
        "#         numeric_values_x = pd.to_numeric(df1[col_x], errors='coerce')\n",
        "#         numeric_values_y = pd.to_numeric(df2[col_y], errors='coerce')\n",
        "#         # print(numeric_values_x ,numeric_values_y)\n",
        "#         print(\"*************\")\n",
        "#         # print(col_x,col_y)\n",
        "#         if numeric_values_x.notna().all() and numeric_values_y.notna().all():\n",
        "#             # Calculate the mean if both columns contain numeric values\n",
        "#             df1[col_y] = (numeric_values_x + numeric_values_y) / 2\n",
        "#         else:\n",
        "#             # If any of the columns contain non-numeric values, use values from the second dataset\n",
        "#             df1[col_y] = df2[col_y].fillna('')\n",
        "#             # If both columns have non-numeric values, just copy from second dataset\n",
        "#             if numeric_values_x.isna().all() and numeric_values_y.isna().all():\n",
        "#                 df1[col_y] = df2[col_y].fillna('')\n",
        "\n",
        "df1.to_csv('combined_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "# //\n",
        "# Combine specified columns from the first dataset\n",
        "# for col_x, col_combined in columns_to_merge.items():\n",
        "#     if col_x in df1.columns:\n",
        "#         df1[col_combined] = df1.apply(lambda x: x[col_x] if pd.notna(x[col_x]) else x[col_combined], axis=1)\n",
        "#         df1 = df1.drop(columns=[col_x])\n",
        "\n",
        "# # Calculate the mean for numeric values\n",
        "# numeric_columns = ['Rating', 'Reviews', 'Size', 'Installs', 'Price']\n",
        "# for col in numeric_columns:\n",
        "#     numeric_values = pd.to_numeric(df1[col], errors='coerce')\n",
        "#     mean_value = numeric_values.mean()\n",
        "#     df1[col] = df1[col].fillna(mean_value)\n",
        "\n",
        "# # Replace non-numeric values with values from the second dataset\n",
        "# non_numeric_indices = df1['Category'].apply(lambda x: not isinstance(x, (int, float)))\n",
        "# df1.loc[non_numeric_indices, 'Category'] = df2.loc[non_numeric_indices, 'Category']\n",
        "\n",
        "# Save the modified DataFrame to a new Excel file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# Read the combined dataset\n",
        "df_combined = pd.read_csv('combined_dataset.csv')\n",
        "\n",
        "# Replace \"Paid\" with 1 and other values with 0 in the \"Type\" column\n",
        "df_combined['Type'] = df_combined['Type'].apply(lambda x: 1 if x == 'Paid' else 0)\n",
        "df_combined['Free'] = df_combined['Free'].apply(lambda x: 1 if x == 'FALSE' else 0)\n",
        "# df_combined['Paid'] =[]*len(df_combined['Free'])\n",
        "\n",
        "df_combined['Paid'] = df_combined.apply(lambda row: 1 if (row['Type'] == 1 or row['Free'] == 1) and row['Price'] != '0' else 0, axis=1)\n",
        "x=['Type','Free']\n",
        "df_combined.drop(columns=x, inplace=True)\n",
        "\n",
        "df_combined.to_csv('combined_dataset.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\BlueWindows\\AppData\\Local\\Temp\\ipykernel_9028\\827602664.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_combined[col] = pd.to_datetime(df_combined[col], errors='coerce')\n",
            "C:\\Users\\BlueWindows\\AppData\\Local\\Temp\\ipykernel_9028\\827602664.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_combined[col] = pd.to_datetime(df_combined[col], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "# df_combined = pd.read_csv('combined_dataset.csv')\n",
        "\n",
        "# columns_to=['Last update','Last Updated']\n",
        "\n",
        "# for i in columns_to:\n",
        "#     values_x = pd.to_datetime(df_combined[i[0]], errors='coerce')\n",
        "#     values_y = pd.to_datetime(df_combined[i[1]], errors='coerce')\n",
        "#     if values_y==values_x:\n",
        "#         df_combined['last update'] = df_combined.apply(values_y, axis=1)\n",
        "#     else:\n",
        "#         df_combined['last update'] = df_combined.apply(values_y, axis=1)\n",
        "          \n",
        "\n",
        "# df_combined.drop(columns=['Last Updated', 'Last update'], inplace=True)\n",
        "\n",
        "# # Save the modified dataset\n",
        "# df_combined.to_csv('combined_dataset-x.csv', index=False)\n",
        "\n",
        "# Read the combined dataset\n",
        "df_combined = pd.read_csv('combined_dataset.csv')\n",
        "\n",
        "# Define columns to work with\n",
        "columns_to = ['Last update', 'Last Updated']\n",
        "\n",
        "# Iterate through the columns\n",
        "for col in columns_to:\n",
        "    # Convert values to datetime\n",
        "    df_combined[col] = pd.to_datetime(df_combined[col], errors='coerce')\n",
        "\n",
        "# Create a new column 'last updates' by taking the maximum value between 'Last update' and 'Last Updated'\n",
        "df_combined['last updates'] = df_combined[['Last update', 'Last Updated']].max(axis=1)\n",
        "\n",
        "# Drop the original columns\n",
        "df_combined.drop(columns=['Last Updated', 'Last update'], inplace=True)\n",
        "\n",
        "# Save the modified dataset\n",
        "df_combined.to_csv('combined_dataset.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('combined_dataset.csv')\n",
        "\n",
        "# Create the new column 'income' by multiplying 'Installs' and 'Price'\n",
        "df['income'] = df['Installs'] * df['Price']\n",
        "\n",
        "# Save the modified CSV file\n",
        "df.to_csv('combined_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = pd.read_csv('combined_dataset.csv')\n",
        "\n",
        "# # Create the new column 'trend' based on the ratio of 'installs' to ('rating count' + 'review')\n",
        "# df['trend'] = df['Installs'] / (df['Rating Count'] + df['Reviews'])\n",
        "\n",
        "# # Save the modified Excel file\n",
        "# df.to_csv('combined_dataset.csv', index=False)\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# # Read the CSV file\n",
        "# df = pd.read_csv('combined_dataset.csv')\n",
        "\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# df['trend'] = df['Installs'] / (df['Rating Count'] + df['Reviews'])\n",
        "\n",
        "# df[\"trend\"] = scaler.fit_transform(df['trend'])\n",
        "\n",
        "# # Save the modified CSV file\n",
        "# df.to_csv('combined_dataset.csv', index=False)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('combined_dataset.csv')\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df['trend'] = df['Installs'] / (df['Rating Count'] + df['Reviews'])\n",
        "# Reshape the 'trend' column to a 2D array\n",
        "trend_values = df['trend'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the 'trend' column\n",
        "scaled_trend = scaler.fit_transform(trend_values)\n",
        "\n",
        "# Multiply the scaled values by 1000\n",
        "scaled_trend = scaled_trend * 1000\n",
        "\n",
        "# Update the 'trend' column in the DataFrame with the scaled and multiplied values\n",
        "df['trend'] = scaled_trend.flatten()\n",
        "\n",
        "# Save the modified CSV file\n",
        "df.to_csv('combined_dataset.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "finish section4 :\n",
        "Combination of two data &\n",
        "remove columns that repeat or have the same contents &\n",
        "Columns with the same contents were inconsistent\n",
        "Replacement method: checking the contents line by line and if they are not the same, I took the data in the reference dataset &\n",
        "create 2 new columns that name are trand and income \n",
        "trand:Installs,Rating Count,Reviews\n",
        "income:Installs,price"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "section 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "I6-Q4LT60yTh",
        "outputId": "675b675b-d357-480a-a370-02580721bf76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\bluewindows\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\bluewindows\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\bluewindows\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bluewindows\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2024.4.28)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bluewindows\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\bluewindows\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\BlueWindows\\AppData\\Local\\Temp\\ipykernel_4224\\942285886.py:18: DtypeWarning: Columns (12,29,30,33,35,37,38,40,42,43,44,46,47,48,49,50,52,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df2 = pd.read_csv('Playstore_final.csv')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "! pip install nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from scipy import stats\n",
        "\n",
        "# Load the datasets\n",
        "df1 = pd.read_csv('GooglePlay.csv')\n",
        "df2 = pd.read_csv('Playstore_final.csv')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-j266iq01IiG"
      },
      "source": [
        "Task 1: Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LC9R3DaH08_W"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m# 50% threshold for missing values\u001b[39;00m\n\u001b[0;32m      7\u001b[0m imputer \u001b[39m=\u001b[39m SimpleImputer(strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m df1[numerical_columns1]\u001b[39m=\u001b[39m imputer\u001b[39m.\u001b[39mfit_transform(df1[numerical_columns1])\n\u001b[0;32m      9\u001b[0m df1 \u001b[39m=\u001b[39m df1\u001b[39m.\u001b[39mdropna(thresh\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df1) \u001b[39m*\u001b[39m threshold, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# for column in df1.columns:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m#     missing = df1[column].isnull().sum()\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "\n",
        "df1 = df1.drop(columns=df1.filter(like='Unnamed').columns)\n",
        "df1 = df1.dropna(subset=['Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price'],inplace=True)\n",
        "#missing\n",
        "numerical_columns1=['Rating', 'Reviews', 'Size', 'Installs','Price']\n",
        "threshold = 0.5 # 50% threshold for missing values\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df1[numerical_columns1]= imputer.fit_transform(df1[numerical_columns1])\n",
        "df1 = df1.dropna(thresh=len(df1) * threshold, axis=1)\n",
        "\n",
        "\n",
        "# for column in df1.columns:\n",
        "#     missing = df1[column].isnull().sum()\n",
        "\n",
        "df2 = df2.dropna(subset=['App Name'\t,'App Id','Category','Rating','Rating Count','Installs','Minimum Installs',\t'Free',\t'Price','Currency',\t'Size',\t'Minimum Android','\tDeveloper Id',\t'Developer Website',\t'Developer Email\tReleased',\t'Last update\tPrivacy Policy',\t'Content Rating',\t'Ad Supported',\t'In app purchases',\t'Editor Choice',\t'Summary',\t'Reviews','\tAndroid version Text',\t'Developer',\t'Developer Address',\t'Developer Internal ID','\tVersion'])\n",
        "numerical_columns=[ 'Rating' ,'Rating Count','Minimum Installs','Price','Installs','Size','Minimum Android','Reviews']\n",
        "#missing\n",
        "# for column in df2.columns:\n",
        "#     missing = df2[column].isnull().sum()\n",
        "    \n",
        "# Impute missing values using mean for numerical columns\n",
        "\n",
        "df2[numerical_columns] = imputer.fit_transform(df2[numerical_columns])\n",
        "\n",
        "# Drop columns with excessive missing values\n",
        "df2 = df2.dropna(thresh=len(df2) * threshold, axis=1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ztTwiLGt1L8a"
      },
      "source": [
        "Task 2: Data Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQu43isS1MOy"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df2[numerical_columns] = scaler.fit_transform(df2[numerical_columns])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xCWxWiTy1UwZ"
      },
      "source": [
        "Task 3: Creating New Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D9DhgCR1Xm3"
      },
      "outputs": [],
      "source": [
        "df2['NewFeature'] = df2['Column1'] + df2['Column2']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y9b8fnio1ZjG"
      },
      "source": [
        "Task 4: Identifying and Removing Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLb77F9F1esD"
      },
      "outputs": [],
      "source": [
        "z_scores = stats.zscore(df2[numerical_columns])\n",
        "abs_z_scores = np.abs(z_scores)\n",
        "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
        "df2 = df2[filtered_entries]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2WJwmSUW1ghe"
      },
      "source": [
        "Task 6: Converting Numerical Data to Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeFQvyUf1qRa"
      },
      "outputs": [],
      "source": [
        "# Convert numerical data to categorical if needed\n",
        "df2['Category'] = pd.cut(df2['NumericColumn'], bins=3, labels=['Low', 'Medium', 'High'])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OHWGQK4415nT"
      },
      "source": [
        "Task 7: Text Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjQAopr-16kq"
      },
      "outputs": [],
      "source": [
        "# Perform text data preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df2['ProcessedText'] = df2['TextColumn'].apply(preprocess_text)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sDx0dyD1194G"
      },
      "source": [
        "Task 8: Statistical Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei8kic8M2B1_"
      },
      "outputs": [],
      "source": [
        "# Statistical comparison between rating of sports genre applications and average overall rating\n",
        "sports_rating = df2[df2['Genre'] == 'Sports']['Rating']\n",
        "overall_rating_mean = df2['Rating'].mean()\n",
        "\n",
        "# Perform statistical test (e.g., t-test) to compare the two ratings\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k-eUTO1K2DjG"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCtJ2m9Z2G7K"
      },
      "outputs": [],
      "source": [
        "# Visualize dataset\n",
        "sns.pairplot(df2[numerical_columns])\n",
        "plt.show()\n",
        "\n",
        "# Visualize categorical data\n",
        "sns.countplot(x='Category', data=df2)\n",
        "plt.show()\n",
        "\n",
        "# Visualize text data\n",
        "wordcloud = WordCloud(width=800, height=400).generate(' '.join(df2['ProcessedText']))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
